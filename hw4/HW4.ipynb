{
 "metadata": {
  "name": "",
  "signature": "sha256:c091a82cb18a688f9f9c41616509fbfbd4ae031a46fd662e15cdaddfb8d05717"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Machine Learning and Computational Statistics, Spring 2105\n",
      "##Charlie Guthrie\n",
      "#Homework 4: Kernels and Duals\n",
      "\n",
      "Due: Tuesday, March 3, 2015, at 4pm\n",
      "\n",
      "##1 Introduction\n",
      "##2 Postiive Semidefinite Matrices\n",
      "1\\. Give an example of an orthogonal matrix that is not symmetric.\n",
      "\n",
      "> $$\\left( \\begin{array}{cc}\n",
      "0 & 1 \\\\\n",
      "-1 & 0 \\end{array} \\right)$$\n",
      "\n",
      "2\\. Use the definition of a psd matrix and the spectral theorem to show that all eigenvalues of a psd matrix are non-negative. \n",
      "\n",
      ">By spectral theorem, $M=Q\\Sigma Q^T$.\n",
      "\n",
      ">Since $Q$ is orthonormal, $Q^TQ=I$ and $Q^T=Q^{-1}$.\n",
      "\n",
      ">Therefore $Q^TM=Q\\Sigma Q^T$ and $Q^TMQ = \\Sigma Q^TQ = \\Sigma$\n",
      "\n",
      ">So for any eigenvector $q$ of matrix $Q$, $q^TMq = \\lambda$, where $\\lambda$ is the corresponding eigenvalue.\n",
      "\n",
      "> By the definition of psd matrices, $q^TMq\\ge0$, and so is $\\lambda$.\n",
      "\n",
      "3\\. In this problem we show that a psd matrix is a matrix version of a non-negative scalar, in that they both have a \u201csquare root\u201d. Show that a symmetric matrix $M$ can be expressed as $M=BB^T$ for some matrix $B$, if and only if $M$ is psd.\n",
      "\n",
      "> Let $M=BB^T$ for some $B$;\n",
      "\n",
      "> And for any $x\\in \\mathbf{R}^n$, let $v = B^Tx$.\n",
      "\n",
      "> Then $v^T = x^TB$ and $x^TMx = x^TBB^Tx = v^Tv \\ge0$\n",
      "\n",
      "> Now the reverse: let $M$ be a psd matrix. Then $M = Q\\Sigma Q^T.  \n",
      "\n",
      "> All eigenvalues of M are non-negative. Therefore there exists a diagonal matrix $X$, where each element on the diagonal is the square root of the corresponding element of $\\Sigma$, such that $X^TX=\\Sigma$.\n",
      "\n",
      "> Then $M$ can be rewritten as $M=QX^TXQ^T=BB^T$, where $B=QX^T$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3 Kernel Matrices\n",
      "1. Consider a set of vectors $S = \\{x_1, . . . , x_m\\}$. Let $X$ denote the matrix whose rows are these vectors. Form the Gram matrix $K = XX^T$ . Show that knowing $K$ is equivalent to knowing the set of pairwise distances among the vectors in $S$ as well as the vector lengths."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Note that $K_{i,j}=x_i^Tx_j$ for any $x_i,x_j\\in S$.\n",
      "\n",
      ">The distance between any two vectors in $S$ is given by $d(x_i,x_j) = ||x_i - x_j||$\n",
      "\n",
      ">$= \\sqrt{(x_i - x_j)^T(x_i - x_j)}$\n",
      "\n",
      ">$= \\sqrt{x_i^Tx_i + x_j^Tx_j - x_i^Tx_j - x_j^Tx_i}$\n",
      "\n",
      ">$= \\sqrt{K_{ii} + K_{jj} - K_{ij} - K_{ji}}$\n",
      "\n",
      "\n",
      "\n",
      "> The vector length of any $x_i \\in S$ is $||x|| = \\sqrt{x_i^Tx_i} = \\sqrt{K_{ii}}$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##4 Kernel Ridge Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1\\. Give an expression for the prediction $f(x) = x^T w^\u2217$ for a new point $x$, not in the training set. The expression should only involve $x$ via inner products with other $x$\u2019s. \n",
      "> $f(x) = x^Tw^* = x^TX^T\\alpha = k_x^T\\alpha$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#5 Novelty Detection\n",
      "\n",
      "A novelty detection algorithm can be based on an algorithm that finds the smallest possible\n",
      "sphere containing the data in feature space.\n",
      "\n",
      "1\\. Let $\u03c6 : X \u2192 F$ be our feature map, mapping elements of the input space into our \u201cfeature space\u201d $F$ , which is equipped with an inner product. Formulate the novelty detection algorithm described above as an optimization problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Note: to simplify notation, I will use $x_i$ to refer to an element of $X$ after it has been mapped into the feature space. \n",
      "\n",
      "> Minimize the radius $r$, subject to the constraint that $r \\ge ||x_i - c||$ for all $x_i \\in X$, where $c$ is the vector giving the center of the sphere.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2\\. Give the Lagrangian for this problem, and write an equivalent, unconstrained \u201cinf sup\u201d version of the optimization problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">$$L(r,c,\u03b1) = r^2 - \\sum_{i=1}^n \\alpha_i (r^2 - ||x_i - c||^2) $$\n",
      "\n",
      ">$$p^* = \\text{inf}_{r,c} \\text{sup}_{\\alpha \\succeq 0} [r^2 - \\sum_{i=1}^n \\alpha_i (r^2 - ||x_i - c||^2) ]$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3\\. Show that we have strong duality and thus we will have an equivalent optimization problem if we swap the inf and the sup. [Hint: Use Slater\u2019s qualification conditions.]\n",
      "> The function defining the sphere is convex.\n",
      "\n",
      ">The domain of $f_0$ is open\n",
      "\n",
      ">Is the inequality constraint linear?\n",
      "\n",
      "> Choosing the origin 0 as the sphere's center $c$, let $r = \\max||x_i|| + \\epsilon$, for an arbitrary $\\epsilon>0$.  These are feasible values for $c$ and $r$, and therefore the problem is feasible.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4\\. Solve the inner minimization problem and give the dual optimization problem. [Note: You may find it convenient to define the kernel function $k(x_i,x_j) = \u27e8\u03c6(x_i),\u03c6(x_j)\u27e9$ and to write your final problem in terms of the corresponding kernel matrix $K$ to simplify notation.]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> The dual optimization problem is: \n",
      "$$d^* = \\text{sup}_{\\alpha \\succeq 0} \\text{inf}_{r,c} \\left[r^2 - \\sum_{i=1}^n \\alpha_i \\left(r^2 - \\left\\|x_i - c \\right\\|^2 \\right) \\right]$$\n",
      "\n",
      "> First we solve the inner minimization problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> $$ \\text{inf}_{r,c} [r^2 - \\sum_{i=1}^n \\alpha_i (r^2 - ||x_i - c||^2) ]$$\n",
      "\n",
      "> $$ = \\text{inf}_{r,c} [r^2(1 - \\sum_{i=1}^n \\alpha_i) + \\sum_{i=1}^n \\alpha_i ||x_i - c||^2 ]$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> The minimum occurs where $\\partial_rL=0$ and $\\partial_cL=0$.\n",
      "\n",
      "> $$\\partial_rL=0 \\iff 2r(1 - \\sum_{i=1}^n \\alpha_i ) = 0 \\iff \\sum_{i=1}^n \\alpha_i = 1$$\n",
      "\n",
      "> $$\\partial_cL=2\\sum_{i=1}^n \\alpha_i(c - x_i)=0 \\iff \\sum_{i=1}^n \\alpha_i c = \\sum_{i=1}^n \\alpha_i x_i \\iff c = \\frac{\\sum_{i=1}^n \\alpha_i x_i}{\\sum_{i=1}^n \\alpha_i} = \\sum_{i=1}^n \\alpha_i x_i$$\n",
      "\n",
      "> Therefore $$\\sum_{i=1}^n \\alpha_i^* = 1$$\n",
      "\n",
      "> and $$c^* = \\sum_{i=1}^n \\alpha_i^* x_i$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> subbing in our values for $c$ and $\\alpha$, the dual problem is \n",
      "\n",
      ">$$d^* = \\text{sup}_{\\alpha \\succeq 0} \\sum_{i=1}^n \\alpha_i   \\left\\|x_i - \\sum_{j=1}^n \\alpha_j x_j\\right\\|^2 $$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "5\\. Write an expression for the optimal sphere in terms of the solution to the dual problem.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> By complementary slackness, $$\\sum_{i=1}^n \\alpha_i^* \\left(r^2 - \\left\\|x_i - c^* \\right\\|^2 \\right) = 0$$\n",
      "\n",
      "> Therefore, $$\\sum_{i=1}^n \\alpha_i^* r^2 = \\sum_{i=1}^n \\alpha_i^* \\left\\|x_i - c^* \\right\\|^2$$\n",
      "\n",
      "> The equation of the sphere is thus $$r^2 = \\sum_{i=1}^n \\alpha_i^* \\left\\|x_i - c^* \\right\\|^2$$,"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "6\\. Write down the complementary slackness conditions for this problem, and characterize the points that are the \u201csupport vectors\u201d.\n",
      "> The only condition for complementary slackness is strong duality, which we do have in this problem.  The support vectors are the $x_i$ for which $\\alpha_i$ is non-zero.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "7\\. Briefly explain how you would apply this algorithm in practice to detect \"novel\" instances.\n",
      "\n",
      "> Any new data that lands outside the sphere would be classified as \"novel\".  \n",
      "\n",
      "8\\. [Optional] Redo this problem allowing osme of the data to lie outside of the sphere, where the number of points outside the sphere can be increased or decreased by adjusting a parameter.\n",
      "\n",
      "> The problem becomes: minimize radius $r$ such that  $r = ||x_i - c|| + \\lambda + y_i$ for all $x_i \\in X$, where $c$ is the vector giving the center of the sphere, $\\lambda$ is our parameter, and each $y_i$ is a slack variable.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##6 Feedback (not graded)\n",
      "1. Approximately how long did it take to complete this assignment? \n",
      ">24 hours\n",
      "\n",
      "2. Any other feedback?\n",
      "> I must admit I am slowly but surely grasping the concepts, even though it is exhausting and excruciating.  I have to resist the temptation to bang my head against the wall when I'm stuck.  I learn the most by talking to the other students in CDS.  "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}