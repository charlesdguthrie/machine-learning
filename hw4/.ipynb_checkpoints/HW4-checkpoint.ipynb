{
 "metadata": {
  "name": "",
  "signature": "sha256:d2edf51ac4bf33ad87f6ed5c17aa8172f85fb72dbf8f20b4eeb0778024f85271"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Machine Learning and Computational Statistics, Spring 2105\n",
      "#Homework 4: Kernels and Duals\n",
      "\n",
      "Due: Tuesday, March 3, 2015, at 4pm\n",
      "\n",
      "##1 Introduction\n",
      "##2 Postiive Semidefinite Matrices\n",
      "1\\. Give an example of an orthogonal matrix that is not symmetric.\n",
      "\n",
      "> $\\left( \\begin{array}{cc}\n",
      "0 & 1 \\\\\n",
      "-1 & 0 \\end{array} \\right)$\n",
      "\n",
      "2\\. Use the definition of a psd matrix and the spectral theorem to show that all eigenvalues of a psd matrix are non-negative. \n",
      "\n",
      ">By spectral theorem, $M=Q\\Sigma Q^T$.\n",
      "\n",
      ">Since $Q$ is orthonormal, $Q^TQ=I$ and $Q^T=Q^{-1}$.\n",
      "\n",
      ">Therefore $Q^TM=Q\\Sigma Q^T$ and $Q^TMQ = \\Sigma Q^TQ = \\Sigma$\n",
      "\n",
      ">So for any eigenvector $q$ of matrix $Q$, $q^TMq = \\lambda$, where $\\lambda$ is the corresponding eigenvalue.\n",
      "\n",
      "> By the definition of psd matrices, $q^TMq\\ge0$, and so is $\\lambda$.\n",
      "\n",
      "3\\. In this problem we show that a psd matrix is a matrix version of a non-negative scalar, in that they both have a \u201csquare root\u201d. Show that a symmetric matrix $M$ can be expressed as $M=BB^T$ for some matrix $B$, if and only if $M$ is psd.\n",
      "\n",
      "> Let $M=BB^T$ for some $B$;\n",
      "\n",
      "> And for any $x\\in \\mathbf{R}^n$, let $v = B^Tx$.\n",
      "\n",
      "> Then $v^T = x^TB$ and $x^TMx = x^TBB^Tx = v^Tv \\ge0$\n",
      "\n",
      "> Now the reverse: let $M$ be a psd matrix. Then $M = Q\\Sigma Q^T.  \n",
      "\n",
      "> All eigenvalues of M are non-negative. Therefore there exists a diagonal matrix $X$, where each element on the diagonal is the square root of the corresponding element of $\\Sigma$, such that $X^TX=\\Sigma$.\n",
      "\n",
      "> Then $M$ can be rewritten as $M=QX^TXQ^T=BB^T$, where $B=QX^T$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3 Kernel Matrices\n",
      "1. Consider a set of vectors $S = \\{x_1, . . . , x_m\\}$. Let $X$ denote the matrix whose rows are these vectors. Form the Gram matrix $K = XX^T$ . Show that knowing $K$ is equivalent to knowing the set of pairwise distances among the vectors in $S$ as well as the vector lengths."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Note that $K_{i,j}=x_i^Tx_j$ for any $x_i,x_j\\in S$.\n",
      "\n",
      ">The distance between any two vectors in $S$ is given by $d(x_i,x_j) = ||x_i - x_j|| \n",
      "\n",
      ">$= \\sqrt{(x_i - x_j)^T(x_i - x_j)}$\n",
      "\n",
      ">$= \\sqrt{x_i^Tx_i + x_j^Tx_j - x_i^Tx_j - x_j^Tx_i}$\n",
      "\n",
      ">$= \\sqrt{K_{ii} + K_{jj} - K_{ij} - K_{ji}}$\n",
      "\n",
      "\n",
      "\n",
      "> The vector length of any $x_i \\in S$ is $||x|| = \\sqrt{x_i^Tx_i} = \\sqrt{K_{ii}}$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##4 Kernel Ridge Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1\\. Give an expression for the prediction $f(x) = x^T w^\u2217$ for a new point $x$, not in the training set. The expression should only involve $x$ via inner products with other $x$\u2019s. \n",
      "> $f(x) = x^Tw^* = x^TX^T\\alpha = k_x^T\\alpha$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#5 Novelty Detection\n",
      "\n",
      "A novelty detection algorithm can be based on an algorithm that finds the smallest possible\n",
      "sphere containing the data in feature space.\n",
      "\n",
      "1\\. Let $\u03c6 : X \u2192 F$ be our feature map, mapping elements of the input space into our \u201cfeature space\u201d $F$ , which is equipped with an inner product. Formulate the novelty detection algorithm described above as an optimization problem.\n",
      ">\n",
      "\n",
      "2\\. Give the Lagrangian for this problem, and write an equivalent, unconstrained \u201cinf sup\u201d version of the optimization problem.\n",
      "\n",
      "3\\. Show that we have strong duality and thus we will have an equivalent optimization problem if we swap the inf and the sup. [Hint: Use Slater\u2019s qualification conditions.]\n",
      "\n",
      "4\\. Solve the inner minimization problem and give the dual optimization problem. [Note: You may find it convenient to define the kernel function k(xi,xj) = \u27e8\u03c6(xi),\u03c6(xj)\u27e9 and to write your final problem in terms of the corresponding kernel matrix K to simplify notation.]\n",
      "\n",
      "5\\. Write an expression for the optimal sphere in terms of the solution to the dual problem.\n",
      "\n",
      "6\\. Write down the complementary slackness conditions for this problem, and characterize the points that are the \u201csupport vectors\u201d."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}