{
 "metadata": {
  "name": "",
  "signature": "sha256:71cc975102162800cda0149d4730924076be24cc9188671a8171207877a65aba"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import timeit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "#1 Preliminaries\n",
      "\n",
      "---\n",
      "##1.1 Dataset construction\n",
      "Start by creating a design matrix for regression with $m = 150$ examples, each of dimension $d = 75$.  We will choose a true weight vector $\\theta$ that has only a few non-zero components:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_design_matrix(examples,dimensions,seed=1):\n",
      "    \"\"\"\n",
      "    examples: number of data points\n",
      "    dimensions: number of features\n",
      "    seed: for replicating randomization\n",
      "    \"\"\"\n",
      "    np.random.seed(seed)\n",
      "    X = np.random.rand(examples,dimensions)\n",
      "    theta = np.zeros(dimensions)\n",
      "    np.random.seed(seed)\n",
      "\n",
      "    first_ten = np.random.randint(2, size=10)\n",
      "    first_ten[first_ten==0] = -10\n",
      "    first_ten[first_ten==1] = 10\n",
      "    #theta[0:10] = [10,-10,10,10,-10,-10,10,10,10,-10]\n",
      "    theta[0:10]=first_ten\n",
      "    np.random.seed(seed)\n",
      "    \n",
      "    epsilon = 0.1*np.random.randn(examples)\n",
      "    y = np.dot(X,theta) + epsilon\n",
      "    \n",
      "    return X,y,theta\n",
      "\n",
      "\n",
      "def train_test_split(X,y,n_train,n_val,n_test):\n",
      "    assert n_train+n_val+n_test == 150, \"Train test split doesn't add up\"\n",
      "    X_train = X[:n_train,:]\n",
      "    y_train = y[:n_train]\n",
      "    X_val = X[n_train:n_train+n_val,:]\n",
      "    y_val = y[n_train:n_train+n_val]\n",
      "    X_test = X[n_train+n_val:,:]\n",
      "    y_test = y[n_train+n_val:]\n",
      "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
      "\n",
      "(X_all,y_all,true_theta) = create_design_matrix(150,75)\n",
      "(X_train, y_train, X_val, y_val, X_test, y_test)=train_test_split(X_all,y_all,80,20,50)\n",
      "\n",
      "print \"train\",X_train.shape\n",
      "print \"val\",X_val.shape\n",
      "print \"test\",X_test.shape\n",
      "print \"ytrain\",y_train.shape\n",
      "print \"yval\",y_val.shape\n",
      "print \"ytest\",y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train (80, 75)\n",
        "val (20, 75)\n",
        "test (50, 75)\n",
        "ytrain (80,)\n",
        "yval (20,)\n",
        "ytest (50,)\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "##1.2 Experiments with Ridge Regression\n",
      "1. Run ridge regression on this dataset.  Choose the $\\lambda$ that minimizes the square loss on the validation set. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Ridge\n",
      "from scipy.optimize import minimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ridge(X,y,Lambda):\n",
      "    (N,D) = X.shape\n",
      "    \"\"\"\n",
      "    takes a regularization term Lambda and returns objective function ridge_obj\n",
      "    \"\"\"\n",
      "    def ridge_obj(theta):\n",
      "        return ((np.linalg.norm(np.dot(X,theta) - y))**2)/(2*N) + Lambda*(np.linalg.norm(theta))**2\n",
      "    return ridge_obj\n",
      "\n",
      "def compute_loss(X,y,theta):\n",
      "    (N,D) = X.shape        \n",
      "    \"\"\"\n",
      "    Computes loss for given dataset and weight vector theta\n",
      "    \"\"\"\n",
      "    return ((np.linalg.norm(np.dot(X,theta) - y))**2)/(2*N)\n",
      "\n",
      "def run_ridge_regression():\n",
      "    (N,D) = X.shape\n",
      "    w = np.random.rand(D,1)\n",
      "\n",
      "    #Sklearn implementation\n",
      "    def ridge_regression(alpha,train,val):\n",
      "        ridge_model = Ridge(alpha)\n",
      "        #ridge_model.fit(X, y)\n",
      "    \n",
      "    #Try various Lambdas, optimize theta, and print loss\n",
      "    t=0\n",
      "    loss_opt=lambda_opt=w_opt_opt=np.nan\n",
      "    Lambdas=[]\n",
      "    loss_hist=[]\n",
      "    for i in range(-9,1):\n",
      "        Lambda = 10**i;\n",
      "        Lambdas.append(Lambda)\n",
      "        w_opt = minimize(ridge(X_train,y_train,Lambda), w)\n",
      "        loss=compute_loss(X_val,y_val, w_opt.x)\n",
      "        loss_hist.append(loss)\n",
      "        if t==0 or loss<loss_opt:\n",
      "            loss_opt = loss\n",
      "            lambda_opt = Lambda\n",
      "            w_opt_opt = w_opt.x.copy()\n",
      "        t=t+1  \n",
      "    \n",
      "    return w_opt_opt,Lambdas,loss_hist\n",
      "w_ridge,lambda_ridge,loss_ridge = run_ridge_regression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ridge(X,y,Lambda):\n",
      "    (N,D) = X.shape\n",
      "    \"\"\"\n",
      "    takes a regularization term Lambda and returns objective function ridge_obj\n",
      "    \"\"\"\n",
      "    def ridge_obj(theta):\n",
      "        return ((np.linalg.norm(np.dot(X,theta) - y))**2)/(2*N) + Lambda*(np.linalg.norm(theta))**2\n",
      "    return ridge_obj\n",
      "\n",
      "def compute_loss(X,y,theta):\n",
      "    (N,D) = X.shape        \n",
      "    \"\"\"\n",
      "    Computes loss for given dataset and weight vector theta\n",
      "    \"\"\"\n",
      "    return ((np.linalg.norm(np.dot(X,theta) - y))**2)/(2*N)\n",
      "\n",
      "def run_ridge_regression():\n",
      "    (N,D) = X.shape\n",
      "    w = np.random.rand(D,1)\n",
      "\n",
      "    #Sklearn implementation\n",
      "    def ridge_regression(alpha,train,val):\n",
      "        ridge_model = Ridge(alpha)\n",
      "        #ridge_model.fit(X, y)\n",
      "    \n",
      "    #Try various Lambdas, optimize theta, and print loss\n",
      "    t=0\n",
      "    loss_opt=lambda_opt=w_opt_opt=np.nan\n",
      "    Lambdas=[]\n",
      "    loss_hist=[]\n",
      "    for i in range(-9,1):\n",
      "        Lambda = 10**i;\n",
      "        Lambdas.append(Lambda)\n",
      "        w_opt = minimize(ridge(X_train,y_train,Lambda), w)\n",
      "        loss=compute_loss(X_val,y_val, w_opt.x)\n",
      "        loss_hist.append(loss)\n",
      "        if t==0 or loss<loss_opt:\n",
      "            loss_opt = loss\n",
      "            lambda_opt = Lambda\n",
      "            w_opt_opt = w_opt.x.copy()\n",
      "        t=t+1  \n",
      "\n",
      "    print \"Best Lambda\",lambda_opt\n",
      "    print \"Best Loss\", loss_opt\n",
      "    \n",
      "    return w_opt_opt,Lambdas,loss_hist\n",
      "        \n",
      "w_ridge,lambdas_ridge,loss_hist_ridge = run_ridge_regression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best Lambda 1e-06\n",
        "Best Loss 0.0171323767578\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(lambdas_ridge,loss_hist_ridge)\n",
      "plt.xlabel('Lambda')\n",
      "plt.ylabel('Loss')\n",
      "plt.xscale('log')\n",
      "plt.title('Ridge Regression: Loss vs. Lambda')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEeCAYAAACKQGL2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1078d4950>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate_theta_estimate(truth,est, tolerance=0):\n",
      "    print \"tolerance\",tolerance\n",
      "    false_nonzeros = len(est[(truth==0) & (est!=0)])\n",
      "    false_zeros = len(est[(truth!=0) & (abs(est)<tolerance)])\n",
      "    print \"True theta is zero, estimate is nonzero\",false_nonzeros\n",
      "    print \"True theta is nonzero, estimate is zero\",false_zeros\n",
      "    return false_nonzeros,false_zeros\n",
      "\n",
      "evaluate_theta_estimate(true_theta,w_ridge)\n",
      "evaluate_theta_estimate(true_theta,w_ridge,tolerance=10**-3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tolerance 0\n",
        "True theta is zero, estimate is nonzero 65\n",
        "True theta is nonzero, estimate is zero 0\n",
        "tolerance 0.001\n",
        "True theta is zero, estimate is nonzero 65\n",
        "True theta is nonzero, estimate is zero 0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "(65, 0)"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "#2 Lasso\n",
      "\n",
      "---\n",
      "##2.1 Shooting Algorithm\n",
      "1. Write a function that computes the Lasso solution for a given \u03bb using the shooting algorithm described above. This function should take a starting point for the optimization as a parameter. Run it on the dataset constructed in (1.1), and select the \u03bb that minimizes the square error on the validation set. Report the optimal value of \u03bb found, and the corresponding test error. Plot the validation error vs \u03bb.\n",
      ">The optimal $\\lambda$ found was 10, corresponding test error was 443.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def soft(a,d):\n",
      "    return np.sign(a) * max(abs(a)-d,0)\n",
      "\n",
      "def lasso_shooting(X,y,Lambda, w_init=[], tolerance=1e-5):\n",
      "    \"\"\"\n",
      "    Takes training data and regularization Lambda,\n",
      "    performs coordinate descent for lasso, aka the \"shooting algorithm\",\n",
      "    and returns a vector of estimated weights\n",
      "    \"\"\"\n",
      "    (N,D) = X.shape\n",
      "    \n",
      "    if len(w_init)==0:\n",
      "        w=np.ones(D)\n",
      "    else: w=w_init\n",
      "    \n",
      "    maxIter = 1000\n",
      "    converged = False\n",
      "    iteration=1\n",
      "    while (converged==False) & (iteration<maxIter):\n",
      "        w_old = w.copy()\n",
      "        for j in range(D):\n",
      "            aj=cj=0\n",
      "            for i in range(1,N):\n",
      "                aj=aj+2*X[i,j]**2\n",
      "                cj=cj+2*X[i,j]*(y[i]-np.dot(w,X[i,:])+ w[j]*X[i,j])\n",
      "\n",
      "            w[j] = soft(cj/aj,Lambda/aj)\n",
      "        iteration=iteration+1\n",
      "        converged = (sum(abs(w-w_old)) < tolerance)\n",
      "        \n",
      "    print \"Converged:\",converged,\"Iterations:\",iteration\n",
      "    return w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Lambda_search(lasso_func):\n",
      "    \"\"\"\n",
      "    Runs lasso_shooting on a training set with a variety of \n",
      "    regularization hyperparameters Lambda.  \n",
      "    Selects the lambda that minimizes square error on the validation set.\n",
      "    Plots validation error vs. Lambda\n",
      "    Prints selected lambda along with corresponding test error.\n",
      "    \"\"\"\n",
      "    #loop through array of lambdas\n",
      "    t=0\n",
      "    Lambdas=[]\n",
      "    loss_hist=[]\n",
      "    print \"Starting Loop\"\n",
      "    for i in range(-5,6):\n",
      "        Lambda = 10**i\n",
      "        print \"Lambda\",Lambda\n",
      "        Lambdas.append(Lambda)\n",
      "        w=lasso_func(X_train,y_train,Lambda,w_init=w_ridge)\n",
      "        loss=compute_loss(X_val,y_val,w)\n",
      "        loss_hist.append(loss)\n",
      "        \n",
      "        if t==0 or loss<=loss_opt:\n",
      "            loss_opt = loss\n",
      "            lambda_opt = Lambda\n",
      "            w_opt = w.copy()\n",
      "        t=t+1\n",
      "\n",
      "    test_loss = compute_loss(X_test,y_test, w_opt)\n",
      "    \n",
      "    print \"Best Lambda:\",lambda_opt\n",
      "    print \"Square Loss on Test Data:\", loss_opt\n",
      "    \n",
      "    return w_opt,Lambdas,loss_hist\n",
      "\n",
      "\n",
      "w_lasso,lambdas_lasso,loss_hist_lasso=Lambda_search(lasso_shooting)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(Lambdas_lasso,loss_hist_lasso)\n",
      "plt.xlabel('Lambda')\n",
      "plt.ylabel('Loss')\n",
      "plt.xscale('log')\n",
      "plt.title('Loss vs. Lambda')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "2\\. Analyze the sparsity of your solution, reporting how many components with true value zero have been estimated to be non-zero, and vice-versa.\n",
      ">There were 49 cases where the true value zero has been estimated to be non-zero, but the reverse never happened.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_theta_estimate(true_theta,w_lasso)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "3\\.Implement the homotopy method described above. Compare the runtime for computing the full regularization path (for the same set of \u03bb\u2019s chosen above) using the homotopy method compared to starting with the same intial point every time.\n",
      "> The time has improved from 1m30s per run to 1m07s per run.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_lambda_max(X,y):\n",
      "    lambda_max = 2*np.linalg.norm(X.T*y,np.inf)\n",
      "    return lambda_max\n",
      "\n",
      "def homotopy_lambda_search():\n",
      "    \"\"\"\n",
      "    Runs lasso_shooting on a training set,\n",
      "    using Lambdas chosen through the homotopy method.\n",
      "    Selects the lambda that minimizes square error on the validation set.\n",
      "    Plots validation error vs. Lambda\n",
      "    Prints selected lambda along with corresponding test error.\n",
      "    \"\"\"\n",
      "    (N,D) = X_train.shape\n",
      "    lambda_max = get_lambda_max(X_train,y_train)\n",
      "    log_lambda_max = int(np.ceil(np.log10(lambda_max)))\n",
      "    #loop through array of lambdas\n",
      "    t=0\n",
      "    Lambdas=[]\n",
      "    loss_hist=[]\n",
      "    w=np.zeros(D)\n",
      "    for i in range(log_lambda_max,-5,-1):\n",
      "        w_old=w.copy()\n",
      "        Lambda = 10**i\n",
      "        Lambdas.append(Lambda)\n",
      "        w=lasso_shooting(X_train,y_train,Lambda,w_init=w_old)\n",
      "        loss=compute_loss(X_val,y_val, w)\n",
      "        loss_hist.append(loss)\n",
      "\n",
      "        if t==0 or loss<=loss_opt:\n",
      "            loss_opt = loss\n",
      "            lambda_opt = Lambda\n",
      "            w_opt = w.copy()\n",
      "        t=t+1\n",
      "\n",
      "    test_loss = compute_loss(X_test,y_test, w_opt)\n",
      "    \n",
      "    print \"Best Lambda:\",lambda_opt\n",
      "    print \"Square Loss on Test Data:\", loss_opt\n",
      "    \n",
      "    return w_opt\n",
      "    \n",
      "w_homotopy,lambdas_homotopy,lambdas_loss_hist = homotopy_lambda_search()\n",
      "\n",
      "plt.plot(lambdas_homotopy,loss_hist_homotopy)\n",
      "plt.xlabel('Lambda')\n",
      "plt.ylabel('Loss')\n",
      "plt.xscale('log')\n",
      "plt.title('Homotopy Loss vs. Lambda')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def timeme(func,iterations=1,*args,**kwargs):\n",
      "    \"\"\"\n",
      "    Timer wrapper.  Runs a given function, with arguments,\n",
      "    100 times and displays the average time per run.  \n",
      "    \"\"\"\n",
      "    def wrapper(func, *args, **kwargs):\n",
      "        def wrapped():\n",
      "            return func(*args, **kwargs)\n",
      "        return wrapped\n",
      "    wrapped = wrapper(func,*args,**kwargs)\n",
      "    run_time = float(timeit.timeit(wrapped, number=iterations))/iterations\n",
      "    print \"Avg time to run %s after %i trials: %i seconds per trial\" %(func,iterations,run_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "timeme(Lambda_search,1,lasso_shooting);\n",
      "timeme(homotopy_lambda_search,1);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4\\. Derive matrix expressions for computing $a_j$ and $c_j$\n",
      "> $a_j = 2\\sum\\limits_{i=1}^n x_{ij}^2  =  2X_j^TX_j = 2[X^TX]_{j,j}$\n",
      "\n",
      "---\n",
      "> $c_j = 2\\sum\\limits_{i=1}^n x_{ij}(y_i - \\mathbf w^T\\mathbf x_i + w_jx_{ij})$ \n",
      "\n",
      ">$=2\\sum\\limits_{i=1}^n x_{ij}y_i - 2\\sum\\limits_{i=1}^n x_{ij}\\mathbf w^T\\mathbf x_i + 2\\sum\\limits_{i=1}^n x_{ij}w_jx_{ij}$\n",
      "\n",
      ">$=2X_j^T\\mathbf y - \\sum\\limits_{k=1}^d 2X_k^TX\\mathbf w + 2X_j^TX_jw_j$\n",
      "\n",
      ">$=2[X^T\\mathbf y]_j - \\sum\\limits_{k=1}^d 2[X^TX]_k\\mathbf w + 2[X^TX]_jw_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "5\\. Implement the matrix expressions and measure the speedup to compute the regularization path."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lasso_shooting_matrix(X,y,Lambda, w_init=[], tolerance=1e-5):\n",
      "    \"\"\"\n",
      "    Takes training data and regularization Lambda,\n",
      "    performs coordinate descent for lasso, aka the \"shooting algorithm\",\n",
      "    and returns a vector of estimated weights\n",
      "    \"\"\"\n",
      "\n",
      "    (N,D) = X.shape\n",
      "    \n",
      "    if len(w_init)==0:\n",
      "        w=np.ones(D)\n",
      "    else: w=w_init\n",
      "    \n",
      "    maxIter = 10000\n",
      "    converged = False\n",
      "    iteration=1\n",
      "    a=c=0\n",
      "    while (converged==False) & (iteration<maxIter):\n",
      "        w_old = w.copy()\n",
      "        XTX = np.dot(X.T,X)\n",
      "        XTy = np.dot(X.T,y)\n",
      "        for j in range(D):\n",
      "            a=2*XTX[j,j]\n",
      "            c=2*XTy[j] - sum(2*np.dot(XTX[j,:],w)) + 2*XTX[j,j]*w[j]\n",
      "            w[j]=soft(c/a,Lambda/a)\n",
      "            #X_i_sq = np.apply_along_axis(lambda x: x ** 2,1,X[i,:])\n",
      "            \n",
      "        iteration=iteration+1\n",
      "        converged = (sum(abs(w-w_old)) < tolerance)\n",
      "        \n",
      "    print \"Converged:\",converged,\"Iterations:\",iteration\n",
      "    return w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "timeme(Lambda_search,1,lasso_shooting_matrix);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "##2.3 Feature Correlation\n",
      "\n",
      "1. Derive the relation between $\\hat{\\theta}_i$ and $\\hat{\\theta}_j$, the $i^{th}$ and $j^{th}$ components of the optimal weight vector obtained by solving the Lasso optimization problem.  \n",
      "\n",
      ">Assume in the optimal solution that $\\hat{\\theta}_i=a$ and $\\hat{\\theta}_j=b$\n",
      "\n",
      ">The lasso objective function, below, must minimize both loss and regularization.  \n",
      "\n",
      ">$$\\sum\\limits_{k=1}^n (h(x_k) - y_k)^2 + \\lambda||w||_1$$\n",
      "\n",
      ">First consider the loss, $\\sum\\limits_{k=1}^n (h(x_k) - y_k)^2$, where $h(x_k)=\\mathbf w^Tx_k$\n",
      "\n",
      ">The loss due to $x_i$, $x_j$ is $\\sum\\limits_{k=1}^n (\\hat{\\theta}_iX_{ik} + \\hat{\\theta}_jX_{jk} - y_k)$\n",
      "\n",
      "> Since $X_i=X_j$, this simplifies to $\\sum\\limits_{k=1}^n ((\\hat{\\theta}_i + \\hat{\\theta}_j)X_{ik} - y_k)$\n",
      "\n",
      "> Thus the optimal values $a$ and $b$ must sum to another value $c$ that minimizes this expression $\\sum\\limits_{k=1}^n (cX_{ik} - y_k)$\n",
      "\n",
      "> Next we minimize the lasso regularization component, $\\lambda||w||_1 = \\lambda\\sum\\limits_{l=1}^d|w_l|$\n",
      "\n",
      "> The regularization penalty due to $x_i$, $x_j$ is $|a| + |b|$.  If $a$ and $b$ are of opposite sign, and both are nonzero, then $|a| + |b|$ > $|c| + |0|$, and the regularization penalty is not minimized.  Therefore $a$ and $b$ must be of the same sign and are constrained by optimal value $c$ such that $a+b=c$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2\\. Derive the relation between $\\hat{\\theta}_i$ and $\\hat{\\theta}_j$, the $i^{th}$ and $j^{th}$ components of the optimal weight vector obtained by solving the ridge regression optimization problem.  \n",
      ">The ridge regression objective function, below, must minimize both loss and regularization.  \n",
      "\n",
      ">$$\\sum\\limits_{k=1}^n (h(x_k) - y_k)^2 + \\lambda||w||_2^2$$\n",
      "\n",
      ">The loss component is the same as with lasso, so we must minimize the regularization penalty subject to the same constraint as in lasso, which is that $a + b = c$\n",
      "\n",
      ">The regularization penalty due to $x_i$, $x_j$ is $a^2 + b^2$.  Next I will show that $a^2 + b^2 \\ge (\\frac{c}{2})^2 + (\\frac{c}{2})^2$, and therefore $a$ and $b$ must be equal to $\\frac{c}{2}=\\frac{a+b}{2}$ under these conditions.  \n",
      "\n",
      "> Claim: $a^2 + b^2 \\ge 2(\\frac{c}{2})^2$\n",
      "\n",
      ">Proof: $a^2 + b^2 = a^2 + (c-a)^2 $\n",
      "\n",
      ">$= a^2 + c^2 - 2ac + a^2$\n",
      "\n",
      ">$= 2a^2 + c^2 - 2ac$\n",
      "\n",
      ">$= \\frac{1}{2}(4a^2 - 4ac + 2c^2)$\n",
      "\n",
      ">$=\\frac{1}{2}(2a-c)^2 + \\frac{c^2}{2}$\n",
      "\n",
      ">$=\\frac{1}{2}(2a-c)^2 + 2(\\frac{c}{2})^2$\n",
      "\n",
      "> $\\frac{1}{2}(2a-c)^2 \\ge 0$, therefore $\\frac{1}{2}(2a-c)^2 + 2(\\frac{c}{2})^2 \\ge 2(\\frac{c}{2})^2$\n",
      "\n",
      "> Thus $a$ and $b$ must be equal."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Feedback\n",
      "1. Approximately how long did it take to complete this assignment?\n",
      ">20 hours\n",
      "\n",
      "2. Any other feedback?\n",
      ">This is exhausting.  Thankfully this is my only class this term (I'm also working full-time)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}