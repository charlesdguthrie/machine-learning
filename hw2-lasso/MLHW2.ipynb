{
 "metadata": {
  "name": "",
  "signature": "sha256:d5647ba45d8f78d9c636d9db9c494d101b899b9b22ca3f0c544e2ee95473bde6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import timeit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "#1 Preliminaries\n",
      "\n",
      "---\n",
      "##1.1 Dataset construction\n",
      "Start by creating a design matrix for regression with $m = 150$ examples, each of dimension $d = 75$.  We will choose a true weight vector $\\theta$ that has only a few non-zero components:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_design_matrix(examples,dimensions,seed=1):\n",
      "    \"\"\"\n",
      "    examples: number of data points\n",
      "    dimensions: number of features\n",
      "    seed: for replicating randomization\n",
      "    \"\"\"\n",
      "    np.random.seed(seed)\n",
      "    X = np.random.rand(examples,dimensions)\n",
      "    theta = np.zeros(dimensions)\n",
      "    np.random.seed(seed)\n",
      "\n",
      "    first_ten = np.random.randint(2, size=10)\n",
      "    first_ten[first_ten==0] = -10\n",
      "    first_ten[first_ten==1] = 10\n",
      "    #theta[0:10] = [10,-10,10,10,-10,-10,10,10,10,-10]\n",
      "    theta[0:10]=first_ten\n",
      "    np.random.seed(seed)\n",
      "    \n",
      "    epsilon = 0.1*np.random.randn(examples)\n",
      "    y = np.dot(X,theta) + epsilon\n",
      "    \n",
      "    return X,y,theta\n",
      "\n",
      "\n",
      "def train_test_split(X,y,n_train,n_val,n_test):\n",
      "    assert n_train+n_val+n_test == 150, \"Train test split doesn't add up\"\n",
      "    X_train = X[:n_train,:]\n",
      "    y_train = y[:n_train]\n",
      "    X_val = X[n_train:n_train+n_val,:]\n",
      "    y_val = y[n_train:n_train+n_val]\n",
      "    X_test = X[n_train+n_val:,:]\n",
      "    y_test = y[n_train+n_val:]\n",
      "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
      "\n",
      "(X_all,y_all,true_theta) = create_design_matrix(150,75)\n",
      "(X_train, y_train, X_val, y_val, X_test, y_test)=train_test_split(X_all,y_all,80,20,50)\n",
      "\n",
      "print \"train\",X_train.shape\n",
      "print \"val\",X_val.shape\n",
      "print \"test\",X_test.shape\n",
      "print \"ytrain\",y_train.shape\n",
      "print \"yval\",y_val.shape\n",
      "print \"ytest\",y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train (80, 75)\n",
        "val (20, 75)\n",
        "test (50, 75)\n",
        "ytrain (80,)\n",
        "yval (20,)\n",
        "ytest (50,)\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "##1.2 Experiments with Ridge Regression\n",
      "1. Run ridge regression on this dataset.  Choose the $\\lambda$ that minimizes the square loss on the validation set. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Ridge\n",
      "from scipy.optimize import minimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ridge(X,y,Lambda):\n",
      "    (N,D) = X.shape\n",
      "    \"\"\"\n",
      "    takes a regularization term Lambda and returns objective function ridge_obj\n",
      "    \"\"\"\n",
      "    def ridge_obj(theta):\n",
      "        return ((np.linalg.norm(np.dot(X,theta) - y))**2)/(2*N) + Lambda*(np.linalg.norm(theta))**2\n",
      "    return ridge_obj\n",
      "\n",
      "def compute_loss(X,y,theta):\n",
      "    (N,D) = X.shape        \n",
      "    \"\"\"\n",
      "    Computes loss for given dataset and weight vector theta\n",
      "    \"\"\"\n",
      "    return ((np.linalg.norm(np.dot(X,theta) - y))**2)/(2*N)\n",
      "\n",
      "def run_ridge_regression():\n",
      "    (N,D) = X.shape\n",
      "    w = np.random.rand(D,1)\n",
      "\n",
      "    #Sklearn implementation\n",
      "    def ridge_regression(alpha,train,val):\n",
      "        ridge_model = Ridge(alpha)\n",
      "        #ridge_model.fit(X, y)\n",
      "    \n",
      "    #Try various Lambdas, optimize theta, and print loss\n",
      "    t=0\n",
      "    loss_opt=lambda_opt=w_opt_opt=np.nan\n",
      "    Lambdas=[]\n",
      "    loss_hist=[]\n",
      "    for i in range(-9,1):\n",
      "        Lambda = 10**i;\n",
      "        Lambdas.append(Lambda)\n",
      "        w_opt = minimize(ridge(X_train,y_train,Lambda), w)\n",
      "        loss=compute_loss(X_val,y_val, w_opt.x)\n",
      "        loss_hist.append(loss)\n",
      "        if t==0 or loss<loss_opt:\n",
      "            loss_opt = loss\n",
      "            lambda_opt = Lambda\n",
      "            w_opt_opt = w_opt.x.copy()\n",
      "        t=t+1  \n",
      "    \n",
      "    return w_opt_opt,Lambdas,loss_hist\n",
      "w_ridge,lambda_ridge,loss_ridge = run_ridge_regression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ridge(X,y,Lambda):\n",
      "    (N,D) = X.shape\n",
      "    \"\"\"\n",
      "    takes a regularization term Lambda and returns objective function ridge_obj\n",
      "    \"\"\"\n",
      "    def ridge_obj(theta):\n",
      "        return ((np.linalg.norm(np.dot(X,theta) - y))**2)/(2*N) + Lambda*(np.linalg.norm(theta))**2\n",
      "    return ridge_obj\n",
      "\n",
      "def compute_loss(X,y,theta):\n",
      "    (N,D) = X.shape        \n",
      "    \"\"\"\n",
      "    Computes loss for given dataset and weight vector theta\n",
      "    \"\"\"\n",
      "    return ((np.linalg.norm(np.dot(X,theta) - y))**2)/(2*N)\n",
      "\n",
      "def run_ridge_regression():\n",
      "    (N,D) = X.shape\n",
      "    w = np.random.rand(D,1)\n",
      "\n",
      "    #Sklearn implementation\n",
      "    def ridge_regression(alpha,train,val):\n",
      "        ridge_model = Ridge(alpha)\n",
      "        #ridge_model.fit(X, y)\n",
      "    \n",
      "    #Try various Lambdas, optimize theta, and print loss\n",
      "    t=0\n",
      "    loss_opt=lambda_opt=w_opt_opt=np.nan\n",
      "    Lambdas=[]\n",
      "    loss_hist=[]\n",
      "    for i in range(-9,1):\n",
      "        Lambda = 10**i;\n",
      "        Lambdas.append(Lambda)\n",
      "        w_opt = minimize(ridge(X_train,y_train,Lambda), w)\n",
      "        loss=compute_loss(X_val,y_val, w_opt.x)\n",
      "        loss_hist.append(loss)\n",
      "        if t==0 or loss<loss_opt:\n",
      "            loss_opt = loss\n",
      "            lambda_opt = Lambda\n",
      "            w_opt_opt = w_opt.x.copy()\n",
      "        t=t+1  \n",
      "\n",
      "    print \"Best Lambda\",lambda_opt\n",
      "    print \"Best Loss\", loss_opt\n",
      "    \n",
      "    return w_opt_opt,Lambdas,loss_hist\n",
      "        \n",
      "w_ridge,lambdas_ridge,loss_hist_ridge = run_ridge_regression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best Lambda 1e-06\n",
        "Best Loss 0.0171323767578\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(lambdas_ridge,loss_hist_ridge)\n",
      "plt.xlabel('Lambda')\n",
      "plt.ylabel('Loss')\n",
      "plt.xscale('log')\n",
      "plt.title('Ridge Regression: Loss vs. Lambda')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEeCAYAAACKQGL2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1078d4950>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate_theta_estimate(truth,est, tolerance=0):\n",
      "    print \"tolerance\",tolerance\n",
      "    false_nonzeros = len(est[(truth==0) & (est!=0)])\n",
      "    false_zeros = len(est[(truth!=0) & (abs(est)<tolerance)])\n",
      "    print \"True theta is zero, estimate is nonzero\",false_nonzeros\n",
      "    print \"True theta is nonzero, estimate is zero\",false_zeros\n",
      "    return false_nonzeros,false_zeros\n",
      "\n",
      "evaluate_theta_estimate(true_theta,w_ridge)\n",
      "evaluate_theta_estimate(true_theta,w_ridge,tolerance=10**-3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tolerance 0\n",
        "True theta is zero, estimate is nonzero 65\n",
        "True theta is nonzero, estimate is zero 0\n",
        "tolerance 0.001\n",
        "True theta is zero, estimate is nonzero 65\n",
        "True theta is nonzero, estimate is zero 0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "(65, 0)"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "#2 Lasso\n",
      "\n",
      "---\n",
      "##2.1 Shooting Algorithm\n",
      "1. Write a function that computes the Lasso solution for a given \u03bb using the shooting algorithm described above. This function should take a starting point for the optimization as a parameter. Run it on the dataset constructed in (1.1), and select the \u03bb that minimizes the square error on the validation set. Report the optimal value of \u03bb found, and the corresponding test error. Plot the validation error vs \u03bb.\n",
      ">The optimal $\\lambda$ found was 10, corresponding test error was 443.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def soft(a,d):\n",
      "    return np.sign(a) * max(abs(a)-d,0)\n",
      "\n",
      "def lasso_shooting(X,y,Lambda, w_init=w_ridge, tolerance=1e-5):\n",
      "    \"\"\"\n",
      "    Takes training data and regularization Lambda,\n",
      "    performs coordinate descent for lasso, aka the \"shooting algorithm\",\n",
      "    and returns a vector of estimated weights\n",
      "    \"\"\"\n",
      "    (N,D) = X.shape\n",
      "    \n",
      "    if len(w_init)==0:\n",
      "        w=np.ones(D)\n",
      "    else: w=w_init\n",
      "    \n",
      "    maxIter = 1000\n",
      "    converged = False\n",
      "    iteration=1\n",
      "    while (converged==False) & (iteration<maxIter):\n",
      "        w_old = w.copy()\n",
      "        for j in range(D):\n",
      "            aj=cj=0\n",
      "            for i in range(1,N):\n",
      "                aj=aj+2*X[i,j]**2\n",
      "                cj=cj+2*X[i,j]*(y[i]-np.dot(w,X[i,:])+ w[j]*X[i,j])\n",
      "\n",
      "            w[j] = soft(cj/aj,Lambda/aj)\n",
      "        iteration=iteration+1\n",
      "        converged = (sum(abs(w-w_old)) < tolerance)\n",
      "        \n",
      "    print \"Converged:\",converged,\"Iterations:\",iteration\n",
      "    return w\n",
      "\n",
      "%timeit lasso_shooting(X_train,y_train,10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Converged: True Iterations: 326\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 2\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 2\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 2\n",
        "1 loops, best of 3: 74 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Lambda_search(lasso_func):\n",
      "    \"\"\"\n",
      "    Runs lasso_shooting on a training set with a variety of \n",
      "    regularization hyperparameters Lambda.  \n",
      "    Selects the lambda that minimizes square error on the validation set.\n",
      "    Plots validation error vs. Lambda\n",
      "    Prints selected lambda along with corresponding test error.\n",
      "    \"\"\"\n",
      "    #loop through array of lambdas\n",
      "    t=0\n",
      "    Lambdas=[]\n",
      "    loss_hist=[]\n",
      "    print \"Starting Loop\"\n",
      "    for i in range(-5,6):\n",
      "        Lambda = 10**i\n",
      "        print \"Lambda\",Lambda\n",
      "        Lambdas.append(Lambda)\n",
      "        w=lasso_func(X_train,y_train,Lambda)\n",
      "        loss=compute_loss(X_val,y_val,w)\n",
      "        loss_hist.append(loss)\n",
      "        \n",
      "        if t==0 or loss<=loss_opt:\n",
      "            loss_opt = loss\n",
      "            lambda_opt = Lambda\n",
      "            w_opt = w.copy()\n",
      "        t=t+1\n",
      "\n",
      "    test_loss = compute_loss(X_test,y_test, w_opt)\n",
      "    \n",
      "    print \"Best Lambda:\",lambda_opt\n",
      "    print \"Square Loss on Test Data:\", loss_opt\n",
      "    \n",
      "    return w_opt,Lambdas,loss_hist\n",
      "\n",
      "\n",
      "w_lasso,lambdas_lasso,loss_hist_lasso=Lambda_search(lasso_shooting)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting Loop\n",
        "Lambda 1e-05\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.0001\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.001\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.01\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.1\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 1\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 357\n",
        "Lambda 10\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 148\n",
        "Lambda 100\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 142\n",
        "Lambda 1000\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 251\n",
        "Lambda 10000\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 3\n",
        "Lambda 100000\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 2\n",
        "Best Lambda: 1\n",
        "Square Loss on Test Data: 0.00607817992338\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(lambdas_lasso,loss_hist_lasso)\n",
      "plt.xlabel('Lambda')\n",
      "plt.ylabel('Loss')\n",
      "plt.xscale('log')\n",
      "plt.xlim((1e-5,1e2))\n",
      "plt.ylim((0,20))\n",
      "plt.title('Lasso Shooting: Loss vs. Lambda')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEeCAYAAACKQGL2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1079fb190>"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "2\\. Analyze the sparsity of your solution, reporting how many components with true value zero have been estimated to be non-zero, and vice-versa.\n",
      ">There were 7 cases where the true value zero has been estimated to be non-zero, but the reverse never happened.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_theta_estimate(true_theta,w_lasso)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tolerance 0\n",
        "True theta is zero, estimate is nonzero 7\n",
        "True theta is nonzero, estimate is zero 0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "(7, 0)"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "3\\.Implement the homotopy method described above. Compare the runtime for computing the full regularization path (for the same set of \u03bb\u2019s chosen above) using the homotopy method compared to starting with the same intial point every time.\n",
      "> The time has improved from 454sec per run to 403sec per run.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_lambda_max(X,y):\n",
      "    lambda_max = 2*np.linalg.norm(X.T*y,np.inf)\n",
      "    return lambda_max\n",
      "\n",
      "def homotopy_lambda_search():\n",
      "    \"\"\"\n",
      "    Runs lasso_shooting on a training set,\n",
      "    using Lambdas chosen through the homotopy method.\n",
      "    Selects the lambda that minimizes square error on the validation set.\n",
      "    Plots validation error vs. Lambda\n",
      "    Prints selected lambda along with corresponding test error.\n",
      "    \"\"\"\n",
      "    (N,D) = X_train.shape\n",
      "    lambda_max = get_lambda_max(X_train,y_train)\n",
      "    log_lambda_max = int(np.ceil(np.log10(lambda_max)))\n",
      "    #loop through array of lambdas\n",
      "    t=0\n",
      "    Lambdas=[]\n",
      "    loss_hist=[]\n",
      "    w=np.zeros(D)\n",
      "    for i in range(log_lambda_max,-5,-1):\n",
      "        w_old=w.copy()\n",
      "        Lambda = 10**i\n",
      "        print \"Lamba\",Lambda\n",
      "        Lambdas.append(Lambda)\n",
      "        w=lasso_shooting(X_train,y_train,Lambda,w_init=w_old)\n",
      "        loss=compute_loss(X_val,y_val, w)\n",
      "        loss_hist.append(loss)\n",
      "\n",
      "        if t==0 or loss<=loss_opt:\n",
      "            loss_opt = loss\n",
      "            lambda_opt = Lambda\n",
      "            w_opt = w.copy()\n",
      "        t=t+1\n",
      "\n",
      "    test_loss = compute_loss(X_test,y_test, w_opt)\n",
      "    \n",
      "    print \"Best Lambda:\",lambda_opt\n",
      "    print \"Square Loss on Test Data:\", loss_opt\n",
      "    \n",
      "    return w_opt,Lambdas,loss_hist\n",
      "    \n",
      "w_homotopy,lambdas_homotopy,loss_hist_homotopy = homotopy_lambda_search()\n",
      "\n",
      "plt.plot(lambdas_homotopy,loss_hist_homotopy)\n",
      "plt.xlabel('Lambda')\n",
      "plt.ylabel('Loss')\n",
      "plt.xscale('log')\n",
      "plt.title('Homotopy Loss vs. Lambda')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def timeme(func,iterations=1,*args,**kwargs):\n",
      "    \"\"\"\n",
      "    Timer wrapper.  Runs a given function, with arguments,\n",
      "    100 times and displays the average time per run.  \n",
      "    \"\"\"\n",
      "    def wrapper(func, *args, **kwargs):\n",
      "        def wrapped():\n",
      "            return func(*args, **kwargs)\n",
      "        return wrapped\n",
      "    wrapped = wrapper(func,*args,**kwargs)\n",
      "    run_time = float(timeit.timeit(wrapped, number=iterations))/iterations\n",
      "    print \"Avg time to run %s after %i trials: %i seconds per trial\" %(func,iterations,run_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "timeme(Lambda_search,1,lasso_shooting);\n",
      "timeme(homotopy_lambda_search,1);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting Loop\n",
        "Lambda 1e-05\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.0001\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.001\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.01\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.1\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 1\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 357\n",
        "Lambda 10\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 148\n",
        "Lambda 100\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 142\n",
        "Lambda 1000\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 251\n",
        "Lambda 10000\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 3\n",
        "Lambda 100000\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 2\n",
        "Best Lambda: 1\n",
        "Square Loss on Test Data: 0.00607817992338\n",
        "Avg time to run <function Lambda_search at 0x107a6d2a8> after 1 trials: 454 seconds per trial\n",
        "Lamba 10000\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 2\n",
        "Lamba 1000\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 251\n",
        "Lamba 100\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 143\n",
        "Lamba 10\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 214\n",
        "Lamba 1\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 411\n",
        "Lamba 0.1\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lamba 0.01\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lamba 0.001\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lamba 0.0001\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Best Lambda: 1\n",
        "Square Loss on Test Data: 0.00607723555481\n",
        "Avg time to run <function homotopy_lambda_search at 0x10796b488> after 1 trials: 403 seconds per trial\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4\\. Derive matrix expressions for computing $a_j$ and $c_j$\n",
      "> $a_j = 2\\sum\\limits_{i=1}^n x_{ij}^2  =  2X_j^TX_j = 2[X^TX]_{j,j}$\n",
      "\n",
      "---\n",
      "> $c_j = 2\\sum\\limits_{i=1}^n x_{ij}(y_i - \\mathbf w^T\\mathbf x_i + w_jx_{ij})$ \n",
      "\n",
      ">$=2\\sum\\limits_{i=1}^n x_{ij}y_i - 2\\sum\\limits_{i=1}^n x_{ij}\\mathbf w^T\\mathbf x_i + 2\\sum\\limits_{i=1}^n x_{ij}w_jx_{ij}$\n",
      "\n",
      ">$=2X_j^T\\mathbf y - \\sum\\limits_{k=1}^d 2X_k^TX\\mathbf w + 2X_j^TX_jw_j$\n",
      "\n",
      ">$=2[X^T\\mathbf y]_j - \\sum\\limits_{k=1}^d 2[X^TX]_k\\mathbf w + 2[X^TX]_jw_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "5\\. Implement the matrix expressions and measure the speedup to compute the regularization path.\n",
      ">Time improved from 454 sec per run to 180 sec per run.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lasso_shooting_matrix(X,y,Lambda, w_init=[], tolerance=1e-5):\n",
      "    \"\"\"\n",
      "    Takes training data and regularization Lambda,\n",
      "    performs coordinate descent for lasso, aka the \"shooting algorithm\",\n",
      "    and returns a vector of estimated weights\n",
      "    \"\"\"\n",
      "\n",
      "    (N,D) = X.shape\n",
      "    \n",
      "    if len(w_init)==0:\n",
      "        w=np.ones(D)\n",
      "    else: w=w_init\n",
      "    \n",
      "    maxIter = 1000\n",
      "    converged = False\n",
      "    iteration=1\n",
      "    a=c=0\n",
      "    while (converged==False) & (iteration<maxIter):\n",
      "        w_old = w.copy()\n",
      "        XTX = np.dot(X.T,X)\n",
      "        XTy = np.dot(X.T,y)\n",
      "        for j in range(D):\n",
      "            a=2*XTX[j,j]\n",
      "            c=2*XTy[j] - 2*np.dot(XTX[j,:],w) + 2*XTX[j,j]*w[j]\n",
      "            w[j]=soft(c/a,Lambda/a)\n",
      "            #X_i_sq = np.apply_along_axis(lambda x: x ** 2,1,X[i,:])\n",
      "            \n",
      "        iteration=iteration+1\n",
      "        converged = (sum(abs(w-w_old)) < tolerance)\n",
      "        \n",
      "    print \"Converged:\",converged,\"Iterations:\",iteration\n",
      "    return w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "timeme(Lambda_search,1,lasso_shooting_matrix);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting Loop\n",
        "Lambda 1e-05\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.0001\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.001\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.01\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.1\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 1\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 341\n",
        "Lambda 10\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 165\n",
        "Lambda 100\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 140\n",
        "Lambda 1000\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 258\n",
        "Lambda 10000\n",
        "Converged: True Iterations: 3\n",
        "Lambda 100000\n",
        "Converged: True Iterations: 2\n",
        "Best Lambda: 0.1\n",
        "Square Loss on Test Data: 0.00655720254995\n",
        "Avg time to run <function Lambda_search at 0x107a6d2a8> after 1 trials: 9 seconds per trial\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w_lasso,lambdas_lasso,loss_hist_lasso = Lambda_search(lasso_shooting_matrix)\n",
      "\n",
      "plt.plot(lambdas_lasso,loss_hist_lasso)\n",
      "plt.xlabel('Lambda')\n",
      "plt.ylabel('Loss')\n",
      "plt.xscale('log')\n",
      "plt.xlim((1e-5,1e2))\n",
      "plt.ylim((0,20))\n",
      "plt.title('Lasso Shooting: Loss vs. Lambda')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting Loop\n",
        "Lambda 1e-05\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.0001\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.001\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.01\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 0.1\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " False Iterations: 1000\n",
        "Lambda 1\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 354\n",
        "Lambda 10\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 165\n",
        "Lambda 100\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 140\n",
        "Lambda 1000\n",
        "Converged:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " True Iterations: 258\n",
        "Lambda 10000\n",
        "Converged: True Iterations: 3\n",
        "Lambda 100000\n",
        "Converged: True Iterations: 2\n",
        "Best Lambda: 1\n",
        "Square Loss on Test Data: 0.00661200914663\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEeCAYAAACKQGL2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkpSu/w/dvH+h328HHgAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x107aa5a90>"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "##2.3 Feature Correlation\n",
      "\n",
      "1. Derive the relation between $\\hat{\\theta}_i$ and $\\hat{\\theta}_j$, the $i^{th}$ and $j^{th}$ components of the optimal weight vector obtained by solving the Lasso optimization problem.  \n",
      "\n",
      ">Assume in the optimal solution that $\\hat{\\theta}_i=a$ and $\\hat{\\theta}_j=b$\n",
      "\n",
      ">The lasso objective function, below, must minimize both loss and regularization.  \n",
      "\n",
      ">$$\\sum\\limits_{k=1}^n (h(x_k) - y_k)^2 + \\lambda||w||_1$$\n",
      "\n",
      ">First consider the loss, $\\sum\\limits_{k=1}^n (h(x_k) - y_k)^2$, where $h(x_k)=\\mathbf w^Tx_k$\n",
      "\n",
      ">The loss due to $x_i$, $x_j$ is $\\sum\\limits_{k=1}^n (\\hat{\\theta}_iX_{ik} + \\hat{\\theta}_jX_{jk} - y_k)$\n",
      "\n",
      "> Since $X_i=X_j$, this simplifies to $\\sum\\limits_{k=1}^n ((\\hat{\\theta}_i + \\hat{\\theta}_j)X_{ik} - y_k)$\n",
      "\n",
      "> Thus the optimal values $a$ and $b$ must sum to another value $c$ that minimizes this expression $\\sum\\limits_{k=1}^n (cX_{ik} - y_k)$\n",
      "\n",
      "> Next we minimize the lasso regularization component, $\\lambda||w||_1 = \\lambda\\sum\\limits_{l=1}^d|w_l|$\n",
      "\n",
      "> The regularization penalty due to $x_i$, $x_j$ is $|a| + |b|$.  If $a$ and $b$ are of opposite sign, and both are nonzero, then $|a| + |b|$ > $|c| + |0|$, and the regularization penalty is not minimized.  Therefore $a$ and $b$ must be of the same sign and are constrained by optimal value $c$ such that $a+b=c$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2\\. Derive the relation between $\\hat{\\theta}_i$ and $\\hat{\\theta}_j$, the $i^{th}$ and $j^{th}$ components of the optimal weight vector obtained by solving the ridge regression optimization problem.  \n",
      ">The ridge regression objective function, below, must minimize both loss and regularization.  \n",
      "\n",
      ">$$\\sum\\limits_{k=1}^n (h(x_k) - y_k)^2 + \\lambda||w||_2^2$$\n",
      "\n",
      ">The loss component is the same as with lasso, so we must minimize the regularization penalty subject to the same constraint as in lasso, which is that $a + b = c$\n",
      "\n",
      ">The regularization penalty due to $x_i$, $x_j$ is $a^2 + b^2$.  Next I will show that $a^2 + b^2 \\ge (\\frac{c}{2})^2 + (\\frac{c}{2})^2$, and therefore $a$ and $b$ must be equal to $\\frac{c}{2}=\\frac{a+b}{2}$ under these conditions.  \n",
      "\n",
      "> Claim: $a^2 + b^2 \\ge 2(\\frac{c}{2})^2$\n",
      "\n",
      ">Proof: $a^2 + b^2 = a^2 + (c-a)^2 $\n",
      "\n",
      ">$= a^2 + c^2 - 2ac + a^2$\n",
      "\n",
      ">$= 2a^2 + c^2 - 2ac$\n",
      "\n",
      ">$= \\frac{1}{2}(4a^2 - 4ac + 2c^2)$\n",
      "\n",
      ">$=\\frac{1}{2}(2a-c)^2 + \\frac{c^2}{2}$\n",
      "\n",
      ">$=\\frac{1}{2}(2a-c)^2 + 2(\\frac{c}{2})^2$\n",
      "\n",
      "> $\\frac{1}{2}(2a-c)^2 \\ge 0$, therefore $\\frac{1}{2}(2a-c)^2 + 2(\\frac{c}{2})^2 \\ge 2(\\frac{c}{2})^2$\n",
      "\n",
      "> Thus $a$ and $b$ must be equal."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Feedback\n",
      "1. Approximately how long did it take to complete this assignment?\n",
      ">20 hours\n",
      "\n",
      "2. Any other feedback?\n",
      ">This is exhausting.  Thankfully this is my only class this term (I'm also working full-time)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}