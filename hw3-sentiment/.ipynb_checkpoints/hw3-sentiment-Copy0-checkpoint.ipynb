{
 "metadata": {
  "name": "",
  "signature": "sha256:3306e877aafe06477f66b2d03195ed737e097fcf01c620c2b685a1c824eba057"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "#Machine Learning and Computational Statistics, Sprint 2015 Homework 3: SVM and Sentiment Analysis\n",
      "---\n",
      "###Charlie Guthrie\n",
      "Due Monday, Feb 23, 2015 at 4pm.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#1 Introduction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Prep\n",
      "import os\n",
      "import numpy as np\n",
      "import pickle\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2 The Data\n",
      "\n",
      "1. Load all the data and randomly split it into 1500 training examples and 500 test examples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_data(file):\n",
      "    '''\n",
      "    Read each file into a list of strings. \n",
      "    Example:\n",
      "    [\"it's\", 'a', 'curious', 'thing', \"i've\", 'found', 'that', 'when', 'willis', 'is', 'not', 'called', 'on', \n",
      "    ...'to', 'carry', 'the', 'whole', 'movie', \"he's\", 'much', 'better', 'and', 'so', 'is', 'the', 'movie']\n",
      "    '''\n",
      "    f = open(file)\n",
      "    lines = f.read().split(' ')\n",
      "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
      "    words = map(lambda Element: Element.translate(None, symbols).strip(), lines)\n",
      "    words = filter(None, words)\n",
      "    return words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#4 Support Vector Machine via Pegasos\n",
      "1. [Written] Compute a subgradient for the \"stochastic\" SVM objective, which assumes a single training point.  Show that if your step size rule is $\\eta_t = 1/(\\lambda t)$, then the corresponding SGD update is the same as given in the pseudocode.  \n",
      ">TODO\n",
      "\n",
      "2. Implement the Pegasos algorithm to run on a sparse data representation. The output should be a sparse weight vector $w$. [As should be your habit, please check your gradient using generic gradient checker while you are in the testing phase. That will be our first question if you ask for help debugging. Once you\u2019re convinced it works, take it out so it doesn\u2019t slow down your code.]\n",
      ">See next cell\n",
      "\n",
      "3. Write a function that takes the sparse weight vector $w$ and a collection of $(x,y)$ pairs, and returns the percent error when predicting $y$ using sign$(wT x)$ (that is, report the 0-1 loss).\n",
      ">See next cell\n",
      "\n",
      "4. Using the bag-of-words feature representation described above, search for the regularization parameter that gives the minimal percent error on your test set. A good search strategy is to start with a set of lambdas spanning a broad range of orders of magnitude. Then, continue to zoom in until you\u2019re convinced that additional search will not significantly improve your test performance. Once you have a sense of the general range of regularization parameters that give good results, you do not have to search over orders of magnitude every time you change something (such as adding new feature).\n",
      ">See next cell\n",
      "\n",
      "5. Recall that the \u201cscore\u201d is the value of the prediction $f(x) = w^Tx$. We like to think that the magnitude of the score represents the confidence of the prediction. This is something we can directly verify or refute. Break the predictions into groups based on the score (you can play with the size of the groups to get a result you think is informative). For each group, examine the percentage error. You can make a table or graph. Summarize the results. Is there a correlation between higher magnitude scores and accuracy?\n",
      ">See next cell"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#############\n",
      "#Question 4.2\n",
      "\n",
      "#############\n",
      "#Question 4.3\n",
      "\n",
      "#############\n",
      "#Question 4.4\n",
      "\n",
      "#############\n",
      "#Question 4.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#5 Error Analysis\n",
      "The natural language processing domain is particularly nice in that one can often interpret why a model has performed well or poorly on a specific example, and sometimes it is not very difficult to come up with ideas for new features that might help fix a problem. The first step in this process is to look closely at the errors that our model makes.\n",
      "\n",
      "1. Choose some examples that the model got wrong. List the features that contributed most heavily to the descision (e.g. rank them by $|w_ix_i|$), along with $x_i, w_i, xw_i$. Do you understand why the model was incorrect? Can you think of a new feature that might be able to fix the issue? Include a short analysis for at least 3 incorrect examples."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#6 Features\n",
      "For a problem like this, the features you use are far more important than the learning model you choose. Whenever you enter a new problem domain, one of your first orders of business is to beg, borrow, or steal the best features you can find. This means looking at any relevant published work and seeing what they\u2019ve used. Maybe it means asking a colleague what features they use. But even- tually you\u2019ll need to engineer new features that help in your particular situation. To get ideas for this dataset, you might check the discussion board on this Kaggle competition, which is using a very similar dataset https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews. There are also a very large number of academic research papers on sentiment analysis that you can look at for ideas.\n",
      "\n",
      "1.  Based on your error analysis, or on some idea you have, find a new feature (or group of features) that improve your test performance. Describe the features and what kind of improvement they give. At this point, it\u2019s important to consider the standard errors $(\\sqrt{\udbff\udc05p(1 \u2212 p)/n})$ on your performance estimates, to know whether the improvement is statistically significant.\n",
      ">TODO\n",
      "\n",
      "2. [Optional] Try to get the best performance possible by generating lots of new features, chang- ing the pre-processing, or any other method you want, so long as you are using the same core SVM model. Describe what you tried, and how much improvement each thing brought to the model. To get you thinking on features, here are some basic ideas of varying quality: 1) how many words are in the review? 2) How many \u201cnegative\u201d words are there? (You\u2019d have to construct or find a list of negative words.) 3) Word n-gram features: Instead of single-word features, you can make every pair of consecutive words a feature. 4) Character n-gram features: Ignore word boundaries and make every sequence of n characters into a feature (this will be a lot). 5) Adding an extra feature whenever a word is preceded by \u201cnot\u201d. For example \u201cnot amazing\u201d becomes its own feature. 6) Do we really need to eliminate those funny characters in the data loading phase? Might there be useful signal there? 7) Use tf-idf instead of raw word counts. The tf-idf is calculated as \n",
      "$$\\textrm{tfidf}(f_i) = \\frac{FF_i}{log(DF_i)}$$\n",
      "where $F F_i$ is the feature frequency of feature $f_i$ and $DF_i$ is the number of document containing $f_i$. In this way we increase the weight of rare words. Sometimes this scheme helps, sometimes it makes things worse. You could try using both! [Extra credit points will be awarded in proportion to how much improvement you achieve.]\n",
      ">TODO if there's time"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#7 Feedback (not graded)\n",
      "1. Approximately how long did it take to complete this assignment?\n",
      ">TODO\n",
      "\n",
      "2. Did you find the Python programming challenging (in particular, converting your code to use sparse representations)?\n",
      ">TODO\n",
      "\n",
      "3. Any other feedback?\n",
      ">TODO"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}